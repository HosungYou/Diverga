# Diverga QA Protocol v2.0
# Scenario MIXED-002: Complex Mixed Methods with Integration Challenges
#
# Purpose: Test C3 mixed methods design with integration strategy discussions,
# Morse notation questions, and practical timeline constraints.
#
# Complexity: HIGH (8-10 turns expected)
# Language: English (user) -> English (response)

scenario_id: MIXED-002
name: "Complex Mixed Methods with Integration Challenges"
version: "2.0"
paradigm: mixed_methods
complexity_level: HIGH

agents_involved:
  - A1-ResearchQuestionRefiner
  - C3-MixedMethodsDesignConsultant
  - E3-MixedMethodsIntegrationSpecialist
  - D1-SamplingStrategyAdvisor
  - D2-InterviewFocusGroupSpecialist

language: "English (user input) -> English (response)"
expected_turns: 8-10
expected_technical_questions: 2
expected_methodological_challenges: 1

checkpoints_expected:
  - id: CP_RESEARCH_DIRECTION
    level: RED
    mandatory_halt: true
    vs_options_min: 4

  - id: CP_INTEGRATION_STRATEGY
    level: RED
    mandatory_halt: true

  - id: CP_TIMELINE_ADJUSTMENT
    level: ORANGE
    mandatory_halt: false

conversation_flow:
  - turn: 1
    user_type: INITIAL_REQUEST
    user: |
      I'm designing a study on AI adoption in higher education.
      I want to use surveys to measure adoption rates and attitudes,
      then interviews to understand the "why" behind the patterns.
      But I'm not sure about the order and integration strategy.

    expected_behavior:
      paradigm_detection: mixed_methods
      checkpoint: CP_RESEARCH_DIRECTION
      halt: true
      vs_options_min: 4
      options_include:
        - "Sequential Explanatory"
        - "Sequential Exploratory"
        - "Convergent Parallel"
        - "Embedded"
      t_scores_visible: true

  - turn: 2
    user_type: TECHNICAL_FOLLOW_UP
    user: |
      Can you explain the Morse notation? I've seen QUAN→qual
      and quan→QUAL but I'm confused about what the capitalization
      and arrows mean in practice.

    expected_behavior:
      maintains_checkpoint: true
      explains_notation: true
      references:
        - "Morse 1991"
        - "priority/dominance"
        - "sequence/timing"
      gives_examples:
        - "QUAN → qual: quantitative dominant, sequential"
        - "QUAL + quan: qualitative dominant, concurrent"
      waits_for_selection: true

  - turn: 3
    user_type: SELECTION
    user: "[A] Sequential Explanatory (QUAN → qual)"

    expected_behavior:
      logs_decision: true
      checkpoint_passed: CP_RESEARCH_DIRECTION
      agent_invoked: E3-MixedMethodsIntegrationSpecialist
      next_checkpoint: CP_INTEGRATION_STRATEGY

  - turn: 4
    user_type: TECHNICAL_FOLLOW_UP
    user: |
      What about joint displays? I've heard they're important for
      integration but I don't really understand how to create one.
      Can you show me an example structure?

    expected_behavior:
      explains_joint_display: true
      provides_example_structure: true
      references:
        - "Creswell & Plano Clark"
        - "side-by-side comparison"
        - "data transformation matrix"
      visual_example: true

  - turn: 5
    user_type: PRACTICAL_CONSTRAINT
    user: |
      Here's my timeline problem: I have 6 months total.
      If I do true sequential (finish QUAN before starting qual),
      I'll run out of time. Can I run them somewhat in parallel
      while still calling it sequential explanatory?

    expected_behavior:
      honest_response: true
      discusses_methodological_tradeoffs: true
      may_suggest:
        - "concurrent with sequential integration"
        - "staggered data collection"
      checkpoint: CP_TIMELINE_ADJUSTMENT
      level: ORANGE
      provides_timeline_advice: true

  - turn: 6
    user_type: METHODOLOGICAL_CHALLENGE
    user: |
      But doesn't running them in parallel violate the sequential
      design? How do I explain that to reviewers without them
      thinking I don't understand mixed methods?

    expected_behavior:
      acknowledges_tension: true
      provides_defense_strategies: true
      references:
        - "pragmatic flexibility"
        - "real-world constraints acknowledgment"
        - "methodological transparency"
      honest_about_limitations: true

  - turn: 7
    user_type: SCOPE_CHANGE
    user: |
      What sample sizes should I target? I'm thinking 200 for
      the survey and 15 for interviews. Does that ratio make sense
      for sequential explanatory?

    expected_behavior:
      agent_invoked: D1-SamplingStrategyAdvisor
      discusses_sample_ratios: true
      considers:
        - "saturation for qualitative"
        - "power analysis for quantitative"
        - "participant selection for interviews"
      provides_recommendations: true

  - turn: 8
    user_type: APPROVAL
    user: |
      This is helpful. Let me confirm: Sequential explanatory with
      staggered timing, 200 survey then 15 purposefully selected
      for interviews, joint display for integration. Approved.

    expected_behavior:
      summarizes_decisions: true
      checkpoint: CP_INTEGRATION_STRATEGY
      level: RED
      final_design_confirmed: true
      suggests_next_steps: true

validation_rules:
  checkpoint_compliance:
    target: 100%
    red_checkpoints_must_halt: true

  technical_depth:
    must_explain_morse_notation: true
    must_provide_joint_display_example: true

  practical_guidance:
    must_address_timeline_constraint: true
    must_discuss_sample_ratios: true

  methodological_honesty:
    must_acknowledge_tradeoffs: true
    must_not_dismiss_concerns: true

metrics_targets:
  total_turns: ">=8"
  technical_questions_handled: ">=2"
  methodological_challenges_handled: ">=1"
  checkpoint_compliance: "100%"
  practical_guidance_provided: true
