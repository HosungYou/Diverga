# Diverga QA Protocol v2.0
# Scenario META-002: Advanced Meta-Analysis with Technical Challenges
#
# Purpose: Test C5/C6/C7 meta-analysis system with technical follow-up questions,
# methodological challenges, and agent transitions.
#
# Complexity: HIGH (10-15 turns expected)
# Language: English (user) -> English (response)

scenario_id: META-002
name: "Advanced Meta-Analysis with Technical Challenges"
version: "2.0"
paradigm: quantitative
complexity_level: HIGH

agents_involved:
  - C5-MetaAnalysisMaster
  - C6-DataIntegrityGuard
  - C7-ErrorPreventionEngine
  - B1-LiteratureReviewStrategist
  - B3-EffectSizeExtractor
  - E1-QuantitativeAnalysisGuide
  - E5-SensitivityAnalysisDesigner
  - A2-TheoreticalFrameworkArchitect

language: "English (user input) -> English (response)"
expected_turns: 10-15
expected_technical_questions: 3
expected_methodological_challenges: 2

checkpoints_expected:
  - id: CP_RESEARCH_DIRECTION
    level: RED
    mandatory_halt: true
    vs_options_min: 3

  - id: CP_METHODOLOGY_APPROVAL
    level: RED
    mandatory_halt: true

  - id: CP_THEORY_SELECTION
    level: ORANGE
    mandatory_halt: true

  - id: CP_SCOPE_DECISION
    level: ORANGE
    mandatory_halt: false

conversation_flow:
  - turn: 1
    user_type: INITIAL_REQUEST
    user: |
      I want to conduct a meta-analysis examining the effectiveness of
      AI-powered tutoring systems on K-12 student learning outcomes.
      I'm particularly interested in understanding whether effects differ
      across STEM versus humanities subjects, and whether learner age
      moderates these effects.

    expected_behavior:
      paradigm_detection: quantitative
      checkpoint: CP_RESEARCH_DIRECTION
      halt: true
      vs_options_min: 3
      t_scores_visible: true
      options_include:
        - "Overall effect analysis"
        - "Subject-specific effects"
        - "Multi-level meta-analysis"

  - turn: 2
    user_type: TECHNICAL_FOLLOW_UP
    user: |
      Before I choose, can you explain why you're recommending Hedges' g
      over Cohen's d? In my field, most studies report Cohen's d.
      What's the practical difference for a meta-analysis?

    expected_behavior:
      maintains_checkpoint: true
      does_not_proceed: true
      provides_technical_explanation: true
      references_expected:
        - "small sample bias"
        - "Hedges correction"
        - "Borenstein"
      waits_for_selection: true

  - turn: 3
    user_type: SELECTION
    user: "[B] Subject-specific effects with age moderation"

    expected_behavior:
      checkpoint_passed: CP_RESEARCH_DIRECTION
      agent_invoked: C5-MetaAnalysisMaster
      logs_decision: true
      next_checkpoint: CP_METHODOLOGY_APPROVAL

  - turn: 4
    user_type: METHODOLOGICAL_CHALLENGE
    user: |
      Wait, I have a concern. Most of my studies have sample sizes under 30.
      Random-effects models assume normality of effect sizes.
      With such small samples, isn't the normality assumption violated?
      Should I use a different approach?

    expected_behavior:
      acknowledges_concern: true
      honest_about_limitations: true
      provides_alternatives:
        - "permutation tests"
        - "robust variance estimation"
        - "Knapp-Hartung correction"
      does_not_dismiss: true
      maintains_methodology_discussion: true

  - turn: 5
    user_type: AGENT_TRANSITION_REQUEST
    user: |
      Actually, before we finalize methodology, I realize I haven't
      established a strong theoretical framework. Can we step back and
      work on that first? I want to ground this in cognitive load theory
      or maybe self-determination theory.

    expected_behavior:
      respects_transition_request: true
      agent_switch: A2-TheoreticalFrameworkArchitect
      preserves_context: true
      checkpoint: CP_THEORY_SELECTION
      vs_options_for_theories: true

  - turn: 6
    user_type: SELECTION
    user: |
      [Cognitive Load Theory] - I think CLT makes more sense because
      AI tutors can reduce extraneous cognitive load.

    expected_behavior:
      logs_theory_decision: true
      returns_to_meta_analysis: true
      agent_switch_back: C5-MetaAnalysisMaster
      context_preserved:
        - "subject-specific effects"
        - "age moderation"
        - "small sample concern"

  - turn: 7
    user_type: SCOPE_CHANGE
    user: |
      One more thing - should I include gray literature like dissertations
      and conference papers? I found 3 relevant dissertations but I'm
      worried about their quality. How does that affect my analysis?

    expected_behavior:
      discusses_publication_bias: true
      references:
        - "funnel plot"
        - "Egger's test"
        - "trim-and-fill"
      provides_decision_framework: true
      checkpoint: CP_SCOPE_DECISION
      level: ORANGE

  - turn: 8
    user_type: ALTERNATIVE_EXPLORATION
    user: |
      What about Bayesian meta-analysis? You didn't mention that as an option.
      Why? Is it not appropriate for educational research?

    expected_behavior:
      explains_bayesian_approach: true
      compares_frequentist_vs_bayesian: true
      honest_about_tradeoffs: true
      may_update_options: true
      t_score_for_bayesian: 0.25  # Innovative option

  - turn: 9
    user_type: PRACTICAL_CONSTRAINT
    user: |
      Here's my concern: I only have 12 studies total, with 4 in STEM
      and 8 in humanities. Is that enough for meaningful subgroup analysis?
      What's the minimum number you'd recommend?

    expected_behavior:
      honest_assessment: true
      references:
        - "minimum k for subgroups"
        - "statistical power in meta-analysis"
        - "Borenstein et al. recommendations"
      may_recommend_alternative: true
      practical_guidance: true

  - turn: 10
    user_type: APPROVAL
    user: |
      OK, I think I understand now. Let's proceed with the random-effects
      model with Knapp-Hartung correction, include gray literature with
      quality sensitivity analysis, and use meta-regression for
      continuous moderators since my subgroups are too small.

    expected_behavior:
      summarizes_decisions: true
      checkpoint: CP_METHODOLOGY_APPROVAL
      level: RED
      presents_final_design: true
      waits_for_approval: true

validation_rules:
  checkpoint_compliance:
    target: 100%
    red_checkpoints_must_halt: true

  technical_depth:
    must_provide_references: true
    must_explain_not_dismiss: true

  context_retention:
    must_remember_across_transitions: true

  language_consistency:
    response_matches_input: true

metrics_targets:
  total_turns: ">=10"
  technical_questions_handled: ">=3"
  methodological_challenges_handled: ">=2"
  agent_transitions: ">=1"
  checkpoint_compliance: "100%"
  context_preserved: ">=95%"
